{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7640924b-86e6-4393-a2f3-53526b7d2ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba19d62c-e783-4b58-9833-cb59fb1b8e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LIGHTKURVE_CACHE'] = \"C:/lkcache\"\n",
    "import lightkurve as lk\n",
    "from astropy.timeseries import BoxLeastSquares\n",
    "from astroquery.mast import Catalogs\n",
    "from astroquery.nasa_exoplanet_archive import NasaExoplanetArchive\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ============================================================================\n",
    "# GET CATALOG PARAMETERS (PERIOD, DEPTH, DURATION)\n",
    "# ============================================================================\n",
    "def get_catalog_transit_params(tic_id, toi_name=None):\n",
    "    \"\"\"\n",
    "    Gets transit depth and duration from catalogs\n",
    "    \"\"\"\n",
    "    \n",
    "    catalog_params = {\n",
    "        'depth': None,\n",
    "        'depth_err': None,\n",
    "        'duration': None,\n",
    "        'duration_err': None\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n  Querying catalogs for transit parameters...\")\n",
    "    \n",
    "    # Looks through TOI catalog \n",
    "    try:\n",
    "        if toi_name is not None:\n",
    "            toi_table = NasaExoplanetArchive.query_criteria(\n",
    "                table=\"TOI\",\n",
    "                select=\"toi, pl_trandep, pl_trandeperr1, pl_trandur, pl_trandurerr1\",\n",
    "                where=f\"toi='{toi_name}'\"\n",
    "            )\n",
    "            \n",
    "            if len(toi_table) > 0:\n",
    "                # Depth in ppm, convert to fraction\n",
    "                if not np.ma.is_masked(toi_table['pl_trandep'][0]) and toi_table['pl_trandep'][0] is not None:\n",
    "                    catalog_params['depth'] = float(toi_table['pl_trandep'][0]) / 1e6  # ppm to fraction\n",
    "                    print(f\"    âœ“ Found catalog depth: {toi_table['pl_trandep'][0]:.1f} ppm\")\n",
    "                \n",
    "                if not np.ma.is_masked(toi_table['pl_trandeperr1'][0]) and toi_table['pl_trandeperr1'][0] is not None:\n",
    "                    catalog_params['depth_err'] = float(toi_table['pl_trandeperr1'][0]) / 1e6\n",
    "                    print(f\"      Error: Â±{toi_table['pl_trandeperr1'][0]:.1f} ppm\")\n",
    "                \n",
    "                # Duration in hours, convert to days\n",
    "                if not np.ma.is_masked(toi_table['pl_trandur'][0]) and toi_table['pl_trandur'][0] is not None:\n",
    "                    catalog_params['duration'] = float(toi_table['pl_trandur'][0]) / 24.0  # hours to days\n",
    "                    print(f\"    âœ“ Found catalog duration: {toi_table['pl_trandur'][0]:.3f} hours\")\n",
    "                \n",
    "                if not np.ma.is_masked(toi_table['pl_trandurerr1'][0]) and toi_table['pl_trandurerr1'][0] is not None:\n",
    "                    catalog_params['duration_err'] = float(toi_table['pl_trandurerr1'][0]) / 24.0\n",
    "                    print(f\"      Error: Â±{toi_table['pl_trandurerr1'][0]:.3f} hours\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"       Could not query TOI catalog: {e}\")\n",
    "    \n",
    "    # Print none if values unknown (missing in catalog)\n",
    "    if catalog_params['depth'] is None and catalog_params['duration'] is None:\n",
    "        print(f\"    error: No transit depth/duration found in catalogs\")\n",
    "    \n",
    "    return catalog_params\n",
    "\n",
    "# ============================================================================\n",
    "# EXTRACTING LIGHT CURVES\n",
    "# ============================================================================\n",
    "def extract_lightcurve_from_ffi(target, sector=None, cutout_size=15):\n",
    "    \"\"\"Extract light curve from TESS Full Frame Images - returns both raw and cleaned light curves\"\"\"\n",
    "    \n",
    "    if isinstance(target, (int, np.integer)):\n",
    "        target = int(target)\n",
    "        target = f\"TIC {target}\"\n",
    "    \n",
    "    print(f\"\\nExtracting FFI light curve for {target}\")\n",
    "    print(\"This may take awhile...\")\n",
    "    \n",
    "    try:\n",
    "        print(\"  Searching for FFI data...\")\n",
    "        search_result = lk.search_tesscut(target)\n",
    "        \n",
    "        if len(search_result) == 0:\n",
    "            print(f\"    No FFI data found\")\n",
    "            return None, None\n",
    "        \n",
    "        if sector is not None:\n",
    "            search_result = search_result[search_result.mission == f'TESS Sector {sector}']\n",
    "            if len(search_result) == 0:\n",
    "                print(f\"    No data in sector {sector}\")\n",
    "                return None, None\n",
    "        \n",
    "        print(f\"    Found FFI data in {len(search_result)} sector(s)\")\n",
    "        \n",
    "        print(f\"  Downloading {cutout_size}x{cutout_size} pixel cutouts...\")\n",
    "        tpf_collection = search_result.download_all(cutout_size=cutout_size)\n",
    "        \n",
    "        if tpf_collection is None or len(tpf_collection) == 0:\n",
    "            print(\"    Download failed\")\n",
    "            return None, None\n",
    "        \n",
    "        lc_list = []\n",
    "        raw_lc_list = []\n",
    "        \n",
    "        for tpf in tpf_collection:\n",
    "            try:\n",
    "                print(f\"  Processing Sector {tpf.sector}...\")\n",
    "                \n",
    "                # Adaptive aperture selection (threshold 5 -> 3 -> circular -> forced radius)\n",
    "                aperture_mask = None\n",
    "                test_mask = tpf.create_threshold_mask(threshold=5, reference_pixel='center')\n",
    "                if 5 <= test_mask.sum() <= 25:\n",
    "                    aperture_mask = test_mask\n",
    "                    print(f\"    Using threshold=5 mask ({test_mask.sum()} pixels)\")\n",
    "                \n",
    "                if aperture_mask is None:\n",
    "                    test_mask = tpf.create_threshold_mask(threshold=3, reference_pixel='center')\n",
    "                    if 5 <= test_mask.sum() <= 30:\n",
    "                        aperture_mask = test_mask\n",
    "                        print(f\"    Using threshold=3 mask ({test_mask.sum()} pixels)\")\n",
    "                \n",
    "                if aperture_mask is None or aperture_mask.sum() == 0:\n",
    "                    print(\"    Using small circular aperture (r=2 pixels)\")\n",
    "                    center_row, center_col = tpf.shape[1]//2, tpf.shape[2]//2\n",
    "                    aperture_mask = np.zeros(tpf.flux[0].shape, dtype=bool)\n",
    "                    for i in range(tpf.shape[1]):\n",
    "                        for j in range(tpf.shape[2]):\n",
    "                            if np.sqrt((i-center_row)**2 + (j-center_col)**2) <= 2:\n",
    "                                aperture_mask[i, j] = True\n",
    "                \n",
    "                if aperture_mask.sum() > 30:\n",
    "                    print(f\"    WARNING: Aperture too large, reducing...\")\n",
    "                    center_row, center_col = tpf.shape[1]//2, tpf.shape[2]//2\n",
    "                    aperture_mask = np.zeros(tpf.flux[0].shape, dtype=bool)\n",
    "                    for i in range(tpf.shape[1]):\n",
    "                        for j in range(tpf.shape[2]):\n",
    "                            if np.sqrt((i-center_row)**2 + (j-center_col)**2) <= 1.5:\n",
    "                                aperture_mask[i, j] = True\n",
    "                \n",
    "                raw_lc = tpf.to_lightcurve(aperture_mask=aperture_mask)\n",
    "                raw_lc = raw_lc.remove_nans()\n",
    "                raw_lc_list.append(raw_lc)\n",
    "                \n",
    "                lc = tpf.to_lightcurve(aperture_mask=aperture_mask)\n",
    "                lc = lc[lc.quality == 0].remove_nans().remove_outliers(sigma=10)\n",
    "                \n",
    "                if len(lc) < 100:\n",
    "                    print(f\"      Too few data points ({len(lc)})\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"      Extracted {len(lc)} data points\")\n",
    "                lc_list.append(lc)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"      Error in Sector {tpf.sector}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(lc_list) == 0:\n",
    "            print(\"    No valid light curves extracted\")\n",
    "            return None, None\n",
    "        \n",
    "        print(f\"\\n  Stitching {len(lc_list)} sector(s) together...\")\n",
    "        \n",
    "        combined_raw_lc = raw_lc_list[0]\n",
    "        for lc in raw_lc_list[1:]:\n",
    "            combined_raw_lc = combined_raw_lc.append(lc)\n",
    "        combined_raw_lc = combined_raw_lc.normalize()\n",
    "        \n",
    "        combined_lc = lc_list[0]\n",
    "        for lc in lc_list[1:]:\n",
    "            combined_lc = combined_lc.append(lc)\n",
    "        combined_lc = combined_lc.normalize().remove_outliers(sigma=7)\n",
    "        \n",
    "        baseline = combined_lc.time[-1].value - combined_lc.time[0].value\n",
    "        print(f\"    Final: {len(combined_lc)} points, {baseline:.1f} day baseline\")\n",
    "        \n",
    "        return combined_lc, combined_raw_lc\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    Extraction failed: {e}\")\n",
    "        return None, None\n",
    "# ============================================================================\n",
    "# SEARCH FOR EXOPLANETS WITH MISSING PLANET RADIUS\n",
    "# ============================================================================\n",
    "def search_tois_with_missing_params():\n",
    "    \"\"\"\n",
    "    Find TOIs with missing orbital parameters\n",
    "    \"\"\"\n",
    "    print(\"=\" * 20)\n",
    "    print(\"SEARCHING FOR TOIs WITH MISSING ORBITAL PARAMETERS\")\n",
    "    print(\"=\" * 20)\n",
    "    \n",
    "    try:\n",
    "        # Check TOI table\n",
    "        toi_table = NasaExoplanetArchive.query_criteria(\n",
    "            table=\"TOI\",\n",
    "            select=\"toi, tid, pl_orbper, pl_orbpererr1, pl_rade, pl_radeerr1, st_tmag\"\n",
    "        )\n",
    "        \n",
    "        df = toi_table.to_pandas()\n",
    "        df = df.dropna(subset=['tid'])\n",
    "        df['tic_id'] = df['tid'].astype(int)\n",
    "        \n",
    "        # Filter for TOIs with MISSING or UNCERTAIN parameters\n",
    "        missing_params = df[\n",
    "            # Has period (transit detected)\n",
    "            (df['pl_orbper'].notna()) &\n",
    "            (\n",
    "                # BUT missing radius\n",
    "                (df['pl_rade'].isna()) |\n",
    "                # OR radius uncertain (>20% error)\n",
    "                (\n",
    "                    (df['pl_radeerr1'].notna()) & \n",
    "                    (df['pl_rade'].notna()) & \n",
    "                    ((df['pl_radeerr1'] / df['pl_rade']) > 0.2)\n",
    "                ) |\n",
    "                # OR period uncertain (>5% error)\n",
    "                (\n",
    "                    (df['pl_orbpererr1'].notna()) & \n",
    "                    ((df['pl_orbpererr1'] / df['pl_orbper']) > 0.05)\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Sort by brightness\n",
    "        missing_params = missing_params.sort_values('st_tmag')\n",
    "        \n",
    "        print(f\"\\nFound {len(missing_params)} TOIs with incomplete/uncertain parameters\")\n",
    "        \n",
    "        # Count different categories\n",
    "        n_missing_radius = missing_params['pl_rade'].isna().sum()\n",
    "        \n",
    "        n_uncertain_radius = 0\n",
    "        if 'pl_radeerr1' in missing_params.columns:\n",
    "            uncertain_mask = (\n",
    "                (missing_params['pl_radeerr1'].notna()) & \n",
    "                (missing_params['pl_rade'].notna()) & \n",
    "                ((missing_params['pl_radeerr1'] / missing_params['pl_rade']) > 0.2)\n",
    "            )\n",
    "            n_uncertain_radius = uncertain_mask.sum()\n",
    "        \n",
    "        n_uncertain_period = 0\n",
    "        if 'pl_orbpererr1' in missing_params.columns:\n",
    "            uncertain_per_mask = (\n",
    "                (missing_params['pl_orbpererr1'].notna()) & \n",
    "                ((missing_params['pl_orbpererr1'] / missing_params['pl_orbper']) > 0.05)\n",
    "            )\n",
    "            n_uncertain_period = uncertain_per_mask.sum()\n",
    "        \n",
    "        print(\"\\nCategories:\")\n",
    "        print(f\"  - Missing radius: {n_missing_radius}\")\n",
    "        print(f\"  - Uncertain radius (>20% error): {n_uncertain_radius}\")\n",
    "        print(f\"  - Uncertain period (>5% error): {n_uncertain_period}\")\n",
    "\n",
    "        # gets top 20 candidates, can change later\n",
    "        print(\"\\nTop 20 candidates (brightest stars):\")\n",
    "        display_cols = ['toi', 'tic_id', 'pl_orbper', 'pl_rade', 'st_tmag']\n",
    "        print(missing_params[display_cols].head(20).to_string(index=False))\n",
    "        \n",
    "        return missing_params\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "# ============================================================================\n",
    "# ESTIMATING ERROR FOR BLS\n",
    "# ============================================================================\n",
    "def estimate_bls_uncertainties(lc, bls_result, best_period, best_duration, best_depth):\n",
    "    \"\"\" Estimate uncertainties in BLS parameters\n",
    "    Period (ðœŽP): estimated from the width of the BLS power peak at half-maximum.\n",
    "    Transit Depth (ðœŽð›¿): calculated using the standard error of the mean flux of in-transit points in the folded light curve.\n",
    "    Duration (ðœŽð‘‘): 10% of the best-fit transit duration (least constrained threshold).\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    best_idx = np.argmin(np.abs(bls_result.period - best_period))\n",
    "    power = bls_result.power\n",
    "    \n",
    "    half_max = (power[best_idx] + np.median(power)) / 2\n",
    "    left_idx = best_idx\n",
    "    right_idx = best_idx\n",
    "    \n",
    "    while left_idx > 0 and power[left_idx] > half_max:\n",
    "        left_idx -= 1\n",
    "    while right_idx < len(power) - 1 and power[right_idx] > half_max:\n",
    "        right_idx += 1\n",
    "    \n",
    "    period_err = (bls_result.period[right_idx] - bls_result.period[left_idx]) / 2\n",
    "    \n",
    "    lc_folded = lc.fold(period=best_period)\n",
    "    phase = lc_folded.time.value\n",
    "    flux = lc_folded.flux.value\n",
    "    \n",
    "    half_dur_phase = (best_duration / best_period) / 2\n",
    "    in_transit = np.abs(phase) < half_dur_phase\n",
    "    \n",
    "    if in_transit.sum() > 5:\n",
    "        depth_err = np.std(flux[in_transit]) / np.sqrt(in_transit.sum())\n",
    "    else:\n",
    "        depth_err = np.std(flux) / np.sqrt(len(flux)) * 2\n",
    "    \n",
    "    duration_err = best_duration * 0.1\n",
    "    \n",
    "    return {\n",
    "        'period_err': period_err,\n",
    "        'depth_err': depth_err,\n",
    "        'duration_err': duration_err\n",
    "    }\n",
    "\n",
    "def compare_with_catalog(bls_results, catalog_period, catalog_period_err=None, \n",
    "                        catalog_depth=None, catalog_depth_err=None,\n",
    "                        catalog_duration=None, catalog_duration_err=None):\n",
    "    \"\"\"Compares BLS results + uncertainties with catalog values (period/depth/duration)\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"UNCERTAINTIES: BLS vs CATALOG\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    bls_period = bls_results['period']\n",
    "    bls_depth = bls_results['depth']\n",
    "    bls_duration = bls_results['duration']\n",
    "    \n",
    "    uncertainties = bls_results['uncertainties']\n",
    "    period_err = uncertainties['period_err']\n",
    "    depth_err = uncertainties['depth_err']\n",
    "    duration_err = uncertainties['duration_err']\n",
    "    \n",
    "    print(\"\\nBLS Results (with uncertainties):\")\n",
    "    print(f\"  Period:   {bls_period:.6f} Â± {period_err:.6f} days\")\n",
    "    print(f\"  Depth:    {bls_depth*1e6:.1f} Â± {depth_err*1e6:.1f} ppm\")\n",
    "    print(f\"  Duration: {bls_duration*24:.3f} Â± {duration_err*24:.3f} hours\")\n",
    "    \n",
    "    print(\"\\nCatalog Values:\")\n",
    "    if catalog_period_err is not None:\n",
    "        print(f\"  Period:   {catalog_period:.6f} Â± {catalog_period_err:.6f} days\")\n",
    "    else:\n",
    "        print(f\"  Period:   {catalog_period:.6f} days (no error available)\")\n",
    "    \n",
    "    if catalog_depth is not None:\n",
    "        if catalog_depth_err is not None:\n",
    "            print(f\"  Depth:    {catalog_depth*1e6:.1f} Â± {catalog_depth_err*1e6:.1f} ppm\")\n",
    "        else:\n",
    "            print(f\"  Depth:    {catalog_depth*1e6:.1f} ppm (no error available)\")\n",
    "    else:\n",
    "        print(f\"  Depth:    Not available\")\n",
    "    \n",
    "    if catalog_duration is not None:\n",
    "        if catalog_duration_err is not None:\n",
    "            print(f\"  Duration: {catalog_duration*24:.3f} Â± {catalog_duration_err*24:.3f} hours\")\n",
    "        else:\n",
    "            print(f\"  Duration: {catalog_duration*24:.3f} hours (no error available)\")\n",
    "    else:\n",
    "        print(f\"  Duration: Not available\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Comparison:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Period comparison\n",
    "    period_diff = bls_period - catalog_period\n",
    "    period_diff_pct = (period_diff / catalog_period) * 100\n",
    "    \n",
    "    if catalog_period_err is not None:\n",
    "        combined_period_err = np.sqrt(period_err**2 + catalog_period_err**2)\n",
    "        period_sigma = abs(period_diff) / combined_period_err if combined_period_err > 0 else np.inf\n",
    "        print(f\"\\nPERIOD:\")\n",
    "        print(f\"  Difference:     {period_diff:+.6f} days ({period_diff_pct:+.2f}%)\")\n",
    "        print(f\"  Combined Error: {combined_period_err:.6f} days\")\n",
    "        print(f\"  Significance:   {period_sigma:.1f} Ïƒ\")\n",
    "    else:\n",
    "        period_sigma = abs(period_diff) / period_err if period_err > 0 else np.inf\n",
    "        print(f\"\\nPERIOD:\")\n",
    "        print(f\"  Difference:     {period_diff:+.6f} days ({period_diff_pct:+.2f}%)\")\n",
    "        print(f\"  Significance:   {period_sigma:.1f} Ïƒ (BLS error)\")\n",
    "    \n",
    "    if period_sigma < 1:\n",
    "        print(f\"  Assessment:     âœ“ Excellent agreement\")\n",
    "    elif period_sigma < 3:\n",
    "        print(f\"  Assessment:     âœ“ Good agreement\")\n",
    "    elif period_sigma < 5:\n",
    "        print(f\"  Assessment:     âš  Slight agreement - need to check more carefully\")\n",
    "    else:\n",
    "        print(f\"  Assessment:     âœ— Poor agreement\")\n",
    "    \n",
    "    # Depth comparison\n",
    "    depth_diff = None\n",
    "    depth_diff_pct = None\n",
    "    depth_sigma = None\n",
    "    if catalog_depth is not None:\n",
    "        depth_diff = bls_depth - catalog_depth\n",
    "        depth_diff_pct = (depth_diff / catalog_depth) * 100\n",
    "        \n",
    "        if catalog_depth_err is not None:\n",
    "            combined_depth_err = np.sqrt(depth_err**2 + catalog_depth_err**2)\n",
    "            depth_sigma = abs(depth_diff) / combined_depth_err if combined_depth_err > 0 else np.inf\n",
    "            print(f\"\\nDEPTH:\")\n",
    "            print(f\"  Difference:     {depth_diff*1e6:+.1f} ppm ({depth_diff_pct:+.1f}%)\")\n",
    "            print(f\"  Combined Error: {combined_depth_err*1e6:.1f} ppm\")\n",
    "            print(f\"  Significance:   {depth_sigma:.1f} Ïƒ\")\n",
    "        else:\n",
    "            depth_sigma = abs(depth_diff) / depth_err if depth_err > 0 else np.inf\n",
    "            print(f\"\\nDEPTH:\")\n",
    "            print(f\"  Difference:     {depth_diff*1e6:+.1f} ppm ({depth_diff_pct:+.1f}%)\")\n",
    "            print(f\"  Significance:   {depth_sigma:.1f} Ïƒ (BLS error)\")\n",
    "        \n",
    "        if depth_sigma < 3:\n",
    "            print(f\"  Assessment:     âœ“ Good agreement\")\n",
    "        elif depth_sigma < 5:\n",
    "            print(f\"  Assessment:     âš  Slight agreement - need to check more carefully\")\n",
    "        else:\n",
    "            print(f\"  Assessment:     âœ— Poor agreement\")\n",
    "    \n",
    "    # Duration comparison\n",
    "    dur_diff = None\n",
    "    dur_diff_pct = None\n",
    "    dur_sigma = None\n",
    "    if catalog_duration is not None:\n",
    "        dur_diff = bls_duration - catalog_duration\n",
    "        dur_diff_pct = (dur_diff / catalog_duration) * 100\n",
    "        \n",
    "        if catalog_duration_err is not None:\n",
    "            combined_dur_err = np.sqrt(duration_err**2 + catalog_duration_err**2)\n",
    "            dur_sigma = abs(dur_diff) / combined_dur_err if combined_dur_err > 0 else np.inf\n",
    "            print(f\"\\nDURATION:\")\n",
    "            print(f\"  Difference:     {dur_diff*24:+.3f} hours ({dur_diff_pct:+.1f}%)\")\n",
    "            print(f\"  Combined Error: {combined_dur_err*24:.3f} hours\")\n",
    "            print(f\"  Significance:   {dur_sigma:.1f} Ïƒ\")\n",
    "        else:\n",
    "            dur_sigma = abs(dur_diff) / duration_err if duration_err > 0 else np.inf\n",
    "            print(f\"\\nDURATION:\")\n",
    "            print(f\"  Difference:     {dur_diff*24:+.3f} hours ({dur_diff_pct:+.1f}%)\")\n",
    "            print(f\"  Significance:   {dur_sigma:.1f} Ïƒ (BLS error)\")\n",
    "        \n",
    "        if dur_sigma < 3:\n",
    "            print(f\"  Assessment:     âœ“ Good agreement\")\n",
    "        elif dur_sigma < 5:\n",
    "            print(f\"  Assessment:     âš  Slight agreement - need to check more carefully\")\n",
    "        else:\n",
    "            print(f\"  Assessment:     âœ— Poor agreement\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return {\n",
    "        'period_diff': period_diff,\n",
    "        'period_diff_pct': period_diff_pct,\n",
    "        'period_sigma': period_sigma,\n",
    "        'depth_diff': depth_diff,\n",
    "        'depth_diff_pct': depth_diff_pct,\n",
    "        'depth_sigma': depth_sigma,\n",
    "        'duration_diff': dur_diff,\n",
    "        'duration_diff_pct': dur_diff_pct,\n",
    "        'duration_sigma': dur_sigma\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# BLS\n",
    "# ============================================================================\n",
    "def search_for_planets_bls(lightcurve, raw_lightcurve=None, known_period=None, \n",
    "                           known_period_err=None, known_depth=None, known_depth_err=None,\n",
    "                           known_duration=None, known_duration_err=None,\n",
    "                           period_min=0.5, period_max=30, plot=True):\n",
    "    \"\"\"Search for transits using BLS with uncertainties/error propagation\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BOX LEAST SQUARES (BLS) SEARCH\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    lc = lightcurve.copy()\n",
    "    \n",
    "    # Detrending light curve\n",
    "    print(\"Detrending light curve...\")\n",
    "    lc = lc.remove_outliers(sigma=5)\n",
    "    \n",
    "    median_cadence = np.median(np.diff(lc.time.value))\n",
    "    \n",
    "    if known_period is not None:\n",
    "        window_length_days = max(2.0, known_period * 3)\n",
    "        print(f\"  Using known period {known_period:.2f}d to set window\")\n",
    "    else:\n",
    "        window_length_days = max(1.0, period_min * 2.5)\n",
    "    \n",
    "    window_length_cadences = int(window_length_days / median_cadence)\n",
    "    if window_length_cadences % 2 == 0:\n",
    "        window_length_cadences += 1\n",
    "    window_length_cadences = max(51, min(window_length_cadences, len(lc) // 3))\n",
    "    \n",
    "    print(f\"  Flattening with window = {window_length_cadences} cadences\")\n",
    "    lc = lc.flatten(window_length=window_length_cadences)\n",
    "    lc = lc.remove_outliers(sigma=4)\n",
    "    \n",
    "    flux_median = np.median(lc.flux.value)\n",
    "    flux_std = np.std(lc.flux.value)\n",
    "    mask_good = np.abs(lc.flux.value - flux_median) < 4 * flux_std\n",
    "    lc = lc[mask_good]\n",
    "    \n",
    "    print(f\"  After cleaning: {len(lc)} points\")\n",
    "    \n",
    "    if len(lc) < 500:\n",
    "        print(\"  Too few points after cleaning\")\n",
    "        return None\n",
    "    \n",
    "    time = lc.time.value\n",
    "    flux = lc.flux.value\n",
    "    flux_err = lc.flux_err.value\n",
    "    \n",
    "    # Adjust period range +/-20 % catalog value\n",
    "    if known_period is not None:\n",
    "        period_min = known_period * 0.8\n",
    "        period_max = known_period * 1.2\n",
    "        print(f\"\\nConstrained search: {period_min:.2f} - {period_max:.2f} days\")\n",
    "    else:\n",
    "        print(f\"\\nUnconstrained search: {period_min:.1f} - {period_max:.1f} days\")\n",
    "    \n",
    "    # Run BLS\n",
    "    print(\"Running BLS...\")\n",
    "    model = BoxLeastSquares(time, flux, flux_err)\n",
    "    periods = np.linspace(period_min, period_max, 10000)\n",
    "    durations = np.linspace(0.02, 0.33, 15)\n",
    "    \n",
    "    bls_result = model.power(periods, durations)\n",
    "    \n",
    "    best_idx = np.argmax(bls_result.power)\n",
    "    \n",
    "    # Estimate uncertainties\n",
    "    print(\"Estimating uncertainties...\")\n",
    "    uncertainties = estimate_bls_uncertainties(\n",
    "        lc, bls_result, \n",
    "        bls_result.period[best_idx],\n",
    "        bls_result.duration[best_idx],\n",
    "        bls_result.depth[best_idx]\n",
    "    )\n",
    "    \n",
    "    results = {\n",
    "        'period': bls_result.period[best_idx],\n",
    "        'duration': bls_result.duration[best_idx],\n",
    "        't0': bls_result.transit_time[best_idx],\n",
    "        'depth': bls_result.depth[best_idx],\n",
    "        'depth_ppm': bls_result.depth[best_idx] * 1e6,\n",
    "        'power': bls_result.power[best_idx],\n",
    "        'snr': bls_result.depth[best_idx] / bls_result.depth_err[best_idx],\n",
    "        'n_transits': int((time[-1] - time[0]) / bls_result.period[best_idx]),\n",
    "        'uncertainties': uncertainties,\n",
    "        'lightcurve': lc,\n",
    "        'raw_lightcurve': raw_lightcurve,\n",
    "        'bls_result': bls_result,\n",
    "        'known_period': known_period,\n",
    "        'known_depth': known_depth,\n",
    "        'known_duration': known_duration\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BLS RESULTS:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  Period:   {results['period']:.6f} Â± {uncertainties['period_err']:.6f} days\")\n",
    "    print(f\"  Duration: {results['duration']*24:.3f} Â± {uncertainties['duration_err']*24:.3f} hours\")\n",
    "    print(f\"  Depth:    {results['depth_ppm']:.1f} Â± {uncertainties['depth_err']*1e6:.1f} ppm\")\n",
    "    print(f\"  BLS Power: {results['power']:.1f}\")\n",
    "    print(f\"  SNR: {results['snr']:.1f}\")\n",
    "    print(f\"  Transits: {results['n_transits']}\")\n",
    "    \n",
    "    # Compare with catalog\n",
    "    if known_period is not None:\n",
    "        comparison = compare_with_catalog(\n",
    "            results, known_period, known_period_err, \n",
    "            known_depth, known_depth_err,\n",
    "            known_duration, known_duration_err\n",
    "        )\n",
    "        results['comparison'] = comparison\n",
    "    \n",
    "    # Plotting graphs (8 in total)\n",
    "    if plot:\n",
    "        fig = plt.figure(figsize=(16, 14))\n",
    "\n",
    "        # 1. Raw light curve\n",
    "        ax0 = plt.subplot(4, 2, 1)\n",
    "        if raw_lightcurve is not None:\n",
    "            raw_lightcurve.scatter(ax=ax0, s=1, c='gray', alpha=0.5)\n",
    "            ax0.set_title('Raw FFI Light Curve', fontsize=12, fontweight='bold')\n",
    "        else:\n",
    "            ax0.text(0.5, 0.5, 'Raw LC not available', ha='center', va='center')\n",
    "        ax0.set_xlabel('Time [BTJD]')\n",
    "        ax0.set_ylabel('Normalized Flux')\n",
    "\n",
    "        # 2. Cleaned/Normalized light curve\n",
    "        ax1 = plt.subplot(4, 2, 2)\n",
    "        lc.scatter(ax=ax1, s=1, c='black', alpha=0.5)\n",
    "        ax1.set_title('Cleaned FFI Light Curve', fontsize=12, fontweight='bold')\n",
    "        ax1.set_xlabel('Time [BTJD]')\n",
    "        ax1.set_ylabel('Normalized Flux')\n",
    "\n",
    "        # 3. BLS power vs trial period (periodogram), with BLS period (red) and catalog period (blue)\n",
    "        ax2 = plt.subplot(4, 2, 3)\n",
    "        ax2.plot(bls_result.period, bls_result.power, 'k-', lw=0.5)\n",
    "        ax2.axvline(results['period'], color='red', ls='--', lw=2, label=f\"BLS: {results['period']:.3f}d\")\n",
    "        if known_period is not None:\n",
    "            ax2.axvline(known_period, color='blue', ls='--', lw=2, alpha=0.5, label=f\"Catalog: {known_period:.3f}d\")\n",
    "        ax2.set_xlabel('Period (days)')\n",
    "        ax2.set_ylabel('BLS Power')\n",
    "        ax2.set_title(f'BLS Periodogram', fontsize=12, fontweight='bold')\n",
    "        ax2.legend()\n",
    "        ax2.grid(alpha=0.3)\n",
    "        ax2.set_xlim(period_min, period_max)\n",
    "\n",
    "        # 4. Folded at BLS period\n",
    "        ax3 = plt.subplot(4, 2, 4)\n",
    "        lc_folded = lc.fold(period=results['period'], epoch_time=results['t0'])\n",
    "        lc_folded.scatter(ax=ax3, s=1, c='black', alpha=0.3)\n",
    "        ax3.set_title(f\"Folded at BLS Period\", fontsize=12, fontweight='bold')\n",
    "        ax3.set_xlabel('Phase')\n",
    "        ax3.set_ylabel('Normalized Flux')\n",
    "        ax3.axvline(0, color='red', ls='--', alpha=0.5)\n",
    "        ax3.grid(alpha=0.3)\n",
    "\n",
    "        # 5. Binned folded at BLS period\n",
    "        ax4 = plt.subplot(4, 2, 5)\n",
    "        lc_folded_binned = lc_folded.bin(time_bin_size=0.01)\n",
    "        lc_folded_binned.scatter(ax=ax4, s=20, c='red')\n",
    "        ax4.set_title(f\"Binned Transit\", fontsize=12, fontweight='bold')\n",
    "        ax4.set_xlabel('Phase')\n",
    "        ax4.set_ylabel('Normalized Flux')\n",
    "        ax4.axvline(0, color='red', ls='--', alpha=0.5)\n",
    "        ax4.grid(alpha=0.3)\n",
    "\n",
    "        # 6. Folded at catalog period\n",
    "        if known_period is not None:\n",
    "            ax5 = plt.subplot(4, 2, 6)\n",
    "            lc_folded_cat = lc.fold(period=known_period)\n",
    "            lc_folded_cat.scatter(ax=ax5, s=1, c='blue', alpha=0.3)\n",
    "            ax5.set_title(f\"Folded at Catalog Period\", fontsize=12, fontweight='bold')\n",
    "            ax5.set_xlabel('Phase')\n",
    "            ax5.set_ylabel('Normalized Flux')\n",
    "            ax5.axvline(0, color='blue', ls='--', alpha=0.5)\n",
    "            ax5.grid(alpha=0.3)\n",
    "            \n",
    "             # 7. Binned folded at catalog period\n",
    "            ax6 = plt.subplot(4, 2, 7)\n",
    "            lc_folded_cat_binned = lc_folded_cat.bin(time_bin_size=0.01)\n",
    "            lc_folded_cat_binned.scatter(ax=ax6, s=20, c='blue')\n",
    "            ax6.set_title(\"Binned at Catalog Period\", fontsize=12, fontweight='bold')\n",
    "            ax6.set_xlabel('Phase')\n",
    "            ax6.set_ylabel('Normalized Flux')\n",
    "            ax6.axvline(0, color='blue', ls='--', alpha=0.5)\n",
    "            ax6.grid(alpha=0.3)\n",
    "\n",
    "        # 8. Zoom in on first transit\n",
    "        ax7 = plt.subplot(4, 2, 8)\n",
    "        if results['n_transits'] > 0:\n",
    "            first_transit = results['t0']\n",
    "            while first_transit < time[0]:\n",
    "                first_transit += results['period']\n",
    "            \n",
    "            zoom_window = results['duration'] * 3\n",
    "            zoom_mask = np.abs(time - first_transit) < zoom_window\n",
    "            \n",
    "            if zoom_mask.sum() > 5:\n",
    "                ax7.plot(time[zoom_mask], flux[zoom_mask], 'k.', ms=2)\n",
    "                ax7.axvline(first_transit, color='red', ls='--', alpha=0.7)\n",
    "                ax7.set_title('Zoom on First Transit', fontsize=12, fontweight='bold')\n",
    "                ax7.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ============================================================================\n",
    "# VETTING\n",
    "# ============================================================================\n",
    "def vet_detection_bls(results):\n",
    "    \"\"\"Vet BLS detection\"\"\"\n",
    "    \n",
    "    period = results['period']\n",
    "    depth_ppm = results['depth_ppm']\n",
    "    duration_hrs = results['duration'] * 24\n",
    "    power = results['power']\n",
    "    snr = results['snr']\n",
    "    n_transits = results['n_transits']\n",
    "    \n",
    "    verdict = \"GOOD\"\n",
    "    reasons = []\n",
    "    \n",
    "    if period < 0.5:\n",
    "        verdict = \"BAD\"\n",
    "        reasons.append(f\"Period too short ({period:.2f}d)\")\n",
    "    \n",
    "    ffi_cadence = 0.0208\n",
    "    for mult in range(1, 10):\n",
    "        if abs(period - mult * ffi_cadence) < 0.002:\n",
    "            verdict = \"BAD\"\n",
    "            reasons.append(f\"Period matches FFI cadence artifact\")\n",
    "            break\n",
    "    \n",
    "    sector_length = 27.4\n",
    "    for mult in [1, 2, 3]:\n",
    "        if abs(period - mult * sector_length) < 2.0:\n",
    "            verdict = \"BAD\"\n",
    "            reasons.append(f\"Period matches sector artifact\")\n",
    "            break\n",
    "    \n",
    "    if depth_ppm > 50000:\n",
    "        verdict = \"BAD\"\n",
    "        reasons.append(f\"Depth too large ({depth_ppm:.0f} ppm)\")\n",
    "    \n",
    "    if depth_ppm < 50 and verdict != \"BAD\":\n",
    "        verdict = \"SUSPICIOUS\"\n",
    "        reasons.append(f\"Very shallow ({depth_ppm:.0f} ppm)\")\n",
    "    \n",
    "    if duration_hrs < 0.5 and verdict != \"BAD\":\n",
    "        verdict = \"SUSPICIOUS\"\n",
    "        reasons.append(f\"Short duration ({duration_hrs:.1f}h)\")\n",
    "    \n",
    "    if duration_hrs > 12 and verdict != \"BAD\":\n",
    "        verdict = \"SUSPICIOUS\"\n",
    "        reasons.append(f\"Long duration ({duration_hrs:.1f}h)\")\n",
    "    \n",
    "    if power < 50:\n",
    "        verdict = \"BAD\"\n",
    "        reasons.append(f\"Low BLS power ({power:.0f})\")\n",
    "    elif power < 100 and verdict != \"BAD\":\n",
    "        verdict = \"SUSPICIOUS\"\n",
    "        reasons.append(f\"Moderate BLS power ({power:.0f})\")\n",
    "    \n",
    "    if snr < 5 and verdict != \"BAD\":\n",
    "        verdict = \"SUSPICIOUS\"\n",
    "        reasons.append(f\"Low SNR ({snr:.1f})\")\n",
    "    \n",
    "    if n_transits < 3 and verdict != \"BAD\":\n",
    "        verdict = \"SUSPICIOUS\"\n",
    "        reasons.append(f\"Only {n_transits} transits\")\n",
    "    \n",
    "    if verdict != \"BAD\" and depth_ppm > 10000:\n",
    "        verdict = \"SUSPICIOUS\"\n",
    "        reasons.append(f\"Large depth ({depth_ppm:.0f} ppm)\")\n",
    "    \n",
    "    if len(reasons) == 0 or (verdict == \"GOOD\"):\n",
    "        reasons.append(f\"Strong detection: P={period:.1f}d, depth={depth_ppm:.0f}ppm, power={power:.0f}\")\n",
    "    \n",
    "    return verdict, reasons\n",
    "\n",
    "def print_vet_result(target, results):\n",
    "    \"\"\"Print vetting results\"\"\"\n",
    "    verdict, reasons = vet_detection_bls(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"VETTING RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if verdict == \"GOOD\":\n",
    "        print(f\"  {target}: âœ“ GOOD CANDIDATE\")\n",
    "    elif verdict == \"SUSPICIOUS\":\n",
    "        print(f\"  {target}: âš  SUSPICIOUS - NEED TO REVIEW MORE\")\n",
    "    else:\n",
    "        print(f\"  {target}: âœ— LIKELY FALSE POSITIVE - DISCARD\")\n",
    "    \n",
    "    print(\"\\nReasons:\")\n",
    "    for reason in reasons:\n",
    "        print(f\"  â€¢ {reason}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# ============================================================================\n",
    "# COMPLETE PIPELINE (1 TOI)\n",
    "# ============================================================================\n",
    "def analyze_missing_radius_toi(tic_id, known_period, known_period_err=None, \n",
    "                                toi_name=None, tmag=None):\n",
    "    \"\"\"Complete analysis pipeline\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ANALYZING TOI {toi_name if toi_name else tic_id}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  TIC: {tic_id}\")\n",
    "    print(f\"  Catalog Period: {known_period:.4f} days\")\n",
    "    if known_period_err is not None:\n",
    "        print(f\"  Catalog Period Error: {known_period_err:.6f} days\")\n",
    "    print(f\"  Radius: MISSING\")\n",
    "    if tmag:\n",
    "        print(f\"  Tmag: {tmag:.2f}\")\n",
    "    \n",
    "    # Get catalog transit parameters \n",
    "    catalog_params = get_catalog_transit_params(tic_id, toi_name)\n",
    "    known_depth = catalog_params['depth']\n",
    "    known_depth_err = catalog_params['depth_err']\n",
    "    known_duration = catalog_params['duration']\n",
    "    known_duration_err = catalog_params['duration_err']\n",
    "    \n",
    "    # Extract light curve\n",
    "    lc, raw_lc = extract_lightcurve_from_ffi(tic_id)\n",
    "    \n",
    "    if lc is None:\n",
    "        print(\"\\nâœ— Failed to extract light curve\")\n",
    "        return None\n",
    "    \n",
    "    # Search with BLS\n",
    "    results = search_for_planets_bls(\n",
    "        lc,\n",
    "        raw_lightcurve=raw_lc,\n",
    "        known_period=known_period,\n",
    "        known_period_err=known_period_err,\n",
    "        known_depth=known_depth,\n",
    "        known_depth_err=known_depth_err,\n",
    "        known_duration=known_duration,\n",
    "        known_duration_err=known_duration_err,\n",
    "        plot=True\n",
    "    )\n",
    "    \n",
    "    if results is None:\n",
    "        print(\"\\nâœ— BLS search failed\")\n",
    "        return None\n",
    "    \n",
    "    print_vet_result(f\"TIC {tic_id}\", results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ============================================================================\n",
    "# BATCH ANALYSIS - MULTIPLE TOIs + SAVES FOLDED LIGHT CURVES TO CSV\n",
    "# ============================================================================\n",
    "def batch_analyze_missing_radius(df_missing, num_candidates=20, save_lcs=True, output_dir='lightcurve_data'):\n",
    "    \"\"\"Batch analyze multiple TOIs + saves folded light curves to csv\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"BATCH ANALYSIS: {num_candidates} CANDIDATES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if save_lcs:\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            print(f\"\\n   Created directory: {output_dir}\")\n",
    "    \n",
    "    results_list = []\n",
    "    saved_lc_count = 0\n",
    "    failure_log = []\n",
    "    \n",
    "    for i in range(min(num_candidates, len(df_missing))):\n",
    "        tic_id = int(df_missing['tic_id'].iloc[i])\n",
    "        known_period = float(df_missing['pl_orbper'].iloc[i])\n",
    "        toi_name = df_missing['toi'].iloc[i]\n",
    "        tmag = df_missing['st_tmag'].iloc[i]\n",
    "        \n",
    "        known_period_err = None\n",
    "        if 'pl_orbpererr1' in df_missing.columns and pd.notna(df_missing['pl_orbpererr1'].iloc[i]):\n",
    "            known_period_err = float(df_missing['pl_orbpererr1'].iloc[i])\n",
    "        \n",
    "        print(f\"\\n[{i+1}/{num_candidates}] Processing {toi_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Get catalog transit parameters\n",
    "            catalog_params = get_catalog_transit_params(tic_id, toi_name)\n",
    "            known_depth = catalog_params['depth']\n",
    "            known_depth_err = catalog_params['depth_err']\n",
    "            known_duration = catalog_params['duration']\n",
    "            known_duration_err = catalog_params['duration_err']\n",
    "            \n",
    "            lc, raw_lc = extract_lightcurve_from_ffi(tic_id)\n",
    "            \n",
    "            if lc is None:\n",
    "                print(f\"  âœ— Failed to extract light curve\")\n",
    "                failure_log.append({'toi': toi_name, 'reason': 'LC extraction failed'})\n",
    "                continue\n",
    "            \n",
    "            results = search_for_planets_bls(\n",
    "                lc,\n",
    "                raw_lightcurve=raw_lc,\n",
    "                known_period=known_period,\n",
    "                known_period_err=known_period_err,\n",
    "                known_depth=known_depth,\n",
    "                known_depth_err=known_depth_err,\n",
    "                known_duration=known_duration,\n",
    "                known_duration_err=known_duration_err,\n",
    "                plot=False\n",
    "            )\n",
    "            \n",
    "            if results is None:\n",
    "                print(f\"  âœ— BLS search failed\")\n",
    "                failure_log.append({'toi': toi_name, 'reason': 'BLS search failed'})\n",
    "                continue\n",
    "            \n",
    "            # Save FOLDED light curve using BLS period\n",
    "            if save_lcs:\n",
    "                try:\n",
    "                    # Use the cleaned light curve from BLS results\n",
    "                    lc_cleaned = results['lightcurve']\n",
    "                    \n",
    "                    # Fold at BLS period\n",
    "                    lc_folded = lc_cleaned.fold(period=results['period'], epoch_time=results['t0'])\n",
    "                    \n",
    "                    # Create dataframe with folded data\n",
    "                    lc_df = pd.DataFrame({\n",
    "                        'phase': lc_folded.time.value,\n",
    "                        'flux_normalized': lc_folded.flux.value,\n",
    "                        'flux_err': lc_folded.flux_err.value\n",
    "                    })\n",
    "                    \n",
    "                    # Sort by phase for easier plotting\n",
    "                    lc_df = lc_df.sort_values('phase').reset_index(drop=True)\n",
    "                    \n",
    "                    filename = f\"{output_dir}/{toi_name.replace('.', '_')}_TIC{tic_id}_folded_lc.csv\"\n",
    "                    lc_df.to_csv(filename, index=False)\n",
    "                    print(f\"    âœ“ Saved folded LC: {len(lc_df)} points (P={results['period']:.4f}d)\")\n",
    "                    saved_lc_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"    âœ— Failed to save folded LC: {e}\")\n",
    "                    failure_log.append({'toi': toi_name, 'reason': f'Save failed: {e}'})\n",
    "            \n",
    "            verdict, _ = vet_detection_bls(results)\n",
    "            \n",
    "            result_dict = {\n",
    "                'toi': toi_name,\n",
    "                'tic_id': tic_id,\n",
    "                'catalog_period': known_period,\n",
    "                'bls_period': results['period'],\n",
    "                'period_err': results['uncertainties']['period_err'],\n",
    "                'depth_ppm': results['depth_ppm'],\n",
    "                'depth_err_ppm': results['uncertainties']['depth_err'] * 1e6,\n",
    "                'duration_hrs': results['duration'] * 24,\n",
    "                'duration_err_hrs': results['uncertainties']['duration_err'] * 24,\n",
    "                'power': results['power'],\n",
    "                'snr': results['snr'],\n",
    "                'n_transits': results['n_transits'],\n",
    "                'verdict': verdict,\n",
    "                'tmag': tmag\n",
    "            }\n",
    "            \n",
    "            if 'comparison' in results:\n",
    "                comp = results['comparison']\n",
    "                result_dict.update({\n",
    "                    'period_diff_pct': comp['period_diff_pct'],\n",
    "                    'period_sigma': comp['period_sigma'],\n",
    "                    'depth_diff_pct': comp['depth_diff_pct'],\n",
    "                    'depth_sigma': comp['depth_sigma'],\n",
    "                    'duration_diff_pct': comp['duration_diff_pct'],\n",
    "                    'duration_sigma': comp['duration_sigma']\n",
    "                })\n",
    "            \n",
    "            results_list.append(result_dict)\n",
    "            \n",
    "            print(f\"  âœ“ P={results['period']:.3f}Â±{results['uncertainties']['period_err']:.4f}d, Power={results['power']:.0f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âœ— Error: {e}\")\n",
    "            failure_log.append({'toi': toi_name, 'reason': f'Exception: {e}'})\n",
    "    \n",
    "    if len(results_list) > 0:\n",
    "        df_results = pd.DataFrame(results_list)\n",
    "        df_results = df_results.sort_values('power', ascending=False)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"BATCH ANALYSIS FINISHED\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nSuccessfully analyzed {len(df_results)}/{num_candidates} targets\\n\")\n",
    "        \n",
    "        print(df_results[['toi', 'tic_id', 'bls_period', 'period_err', \n",
    "                         'depth_ppm', 'power', 'verdict']].to_string(index=False))\n",
    "        \n",
    "        n_good = (df_results['verdict'] == 'GOOD').sum()\n",
    "        n_susp = (df_results['verdict'] == 'SUSPICIOUS').sum()\n",
    "        n_bad = (df_results['verdict'] == 'BAD').sum()\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"VETTING SUMMARY:\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"  âœ“ Good candidates: {n_good}\")\n",
    "        print(f\"  âš  Suspicious: {n_susp}\")\n",
    "        print(f\"  âœ— Likely false positives: {n_bad}\")\n",
    "        \n",
    "        filename = 'missing_radius_bls_results.csv'\n",
    "        df_results.to_csv(filename, index=False)\n",
    "        print(f\"\\n  Results saved to {filename}\")\n",
    "        \n",
    "        if save_lcs:\n",
    "            print(f\"  Successfully saved {saved_lc_count}/{num_candidates} folded light curves to '{output_dir}/'\")\n",
    "        \n",
    "        # Print failure summary\n",
    "        if len(failure_log) > 0:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"FAILURE LOG ({len(failure_log)} targets failed):\")\n",
    "            print(f\"{'='*70}\")\n",
    "            for fail in failure_log:\n",
    "                print(f\"  {fail['toi']}: {fail['reason']}\")\n",
    "        \n",
    "        return df_results\n",
    "    \n",
    "    else:\n",
    "        print(\"\\n   No successful analyses\")\n",
    "        if len(failure_log) > 0:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"FAILURE LOG ({len(failure_log)} targets failed):\")\n",
    "            print(f\"{'='*70}\")\n",
    "            for fail in failure_log:\n",
    "                print(f\"  {fail['toi']}: {fail['reason']}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39b9bc64-ea90-4602-873c-32928f2c5f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING BATCH ANALYSIS ON 2 TOIs\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "SEARCHING FOR TOIs WITH MISSING RADIUS\n",
      "============================================================\n",
      "\n",
      "  Found 507 TOIs with missing radius\n",
      "  Tmag range: 5.5 - 18.3\n",
      "\n",
      "  First 5 candidates:\n",
      "    toi    tic_id  pl_orbper  st_tmag\n",
      "1025.01 297967252   9.683792   5.5324\n",
      "1029.01 374908020  36.222140   6.8410\n",
      "1457.01 176860064   6.376817   7.1145\n",
      "4391.01  66620917   5.291010   7.3010\n",
      "1127.01 269450900   2.324390   7.3163\n",
      "\n",
      "============================================================\n",
      "RUNNING BATCH ANALYSIS ON 2 TOIs...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "BATCH ANALYSIS: 2 CANDIDATES\n",
      "============================================================\n",
      "\n",
      "[1/2] Processing 1025.01...\n",
      "\n",
      "  Querying catalogs for transit parameters...\n",
      "       Could not query TOI catalog: ORA-00904: 'PL_TRANDURERR1': invalid identifier\n",
      "    error: No transit depth/duration found in catalogs\n",
      "\n",
      "Extracting FFI light curve for TIC 297967252\n",
      "This may take awhile...\n",
      "  Searching for FFI data...\n",
      "    Found FFI data in 8 sector(s)\n",
      "  Downloading 15x15 pixel cutouts...\n",
      "  Processing Sector 10...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 1150 data points\n",
      "  Processing Sector 9...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 1084 data points\n",
      "  Processing Sector 36...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 3467 data points\n",
      "  Processing Sector 35...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 2815 data points\n",
      "  Processing Sector 62...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 10811 data points\n",
      "  Processing Sector 63...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 10942 data points\n",
      "  Processing Sector 89...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 12144 data points\n",
      "  Processing Sector 90...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 11560 data points\n",
      "\n",
      "  Stitching 8 sector(s) together...\n",
      "    Final: 53973 points, 2205.6 day baseline\n",
      "\n",
      "============================================================\n",
      "BOX LEAST SQUARES (BLS) SEARCH\n",
      "============================================================\n",
      "Detrending light curve...\n",
      "  Using known period 9.68d to set window\n",
      "  Flattening with window = 12551 cadences\n",
      "  After cleaning: 53830 points\n",
      "\n",
      "Constrained search: 7.75 - 11.62 days\n",
      "Running BLS...\n",
      "Estimating uncertainties...\n",
      "\n",
      "============================================================\n",
      "BLS RESULTS:\n",
      "============================================================\n",
      "  Period:   8.759672 Â± 0.004261 days\n",
      "  Duration: 7.920 Â± 0.792 hours\n",
      "  Depth:    1382.5 Â± 173.2 ppm\n",
      "  BLS Power: 289850.8\n",
      "  SNR: 747.4\n",
      "  Transits: 251\n",
      "\n",
      "============================================================\n",
      "UNCERTAINTIES: BLS vs CATALOG\n",
      "============================================================\n",
      "\n",
      "BLS Results (with uncertainties):\n",
      "  Period:   8.759672 Â± 0.004261 days\n",
      "  Depth:    1382.5 Â± 173.2 ppm\n",
      "  Duration: 7.920 Â± 0.792 hours\n",
      "\n",
      "Catalog Values:\n",
      "  Period:   9.683792 Â± 0.000001 days\n",
      "  Depth:    Not available\n",
      "  Duration: Not available\n",
      "\n",
      "------------------------------------------------------------\n",
      "Comparison:\n",
      "------------------------------------------------------------\n",
      "\n",
      "PERIOD:\n",
      "  Difference:     -0.924120 days (-9.54%)\n",
      "  Combined Error: 0.004261 days\n",
      "  Significance:   216.9 Ïƒ\n",
      "  Assessment:     âœ— Poor agreement\n",
      "============================================================\n",
      "    âœ“ Saved folded LC: 53830 points (P=8.7597d)\n",
      "  âœ“ P=8.760Â±0.0043d, Power=289851\n",
      "\n",
      "[2/2] Processing 1029.01...\n",
      "\n",
      "  Querying catalogs for transit parameters...\n",
      "       Could not query TOI catalog: ORA-00904: 'PL_TRANDURERR1': invalid identifier\n",
      "    error: No transit depth/duration found in catalogs\n",
      "\n",
      "Extracting FFI light curve for TIC 374908020\n",
      "This may take awhile...\n",
      "  Searching for FFI data...\n",
      "    Found FFI data in 37 sector(s)\n",
      "  Downloading 15x15 pixel cutouts...\n",
      "  Processing Sector 6...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 980 data points\n",
      "  Processing Sector 5...\n",
      "    Using threshold=3 mask (30 pixels)\n",
      "      Extracted 1176 data points\n",
      "  Processing Sector 4...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 1027 data points\n",
      "  Processing Sector 3...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 1077 data points\n",
      "  Processing Sector 2...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 1196 data points\n",
      "  Processing Sector 10...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 1150 data points\n",
      "  Processing Sector 13...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 1283 data points\n",
      "  Processing Sector 12...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 1234 data points\n",
      "  Processing Sector 7...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 1086 data points\n",
      "  Processing Sector 9...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 1084 data points\n",
      "  Processing Sector 27...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 3351 data points\n",
      "  Processing Sector 30...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 3681 data points\n",
      "  Processing Sector 33...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 3485 data points\n",
      "  Processing Sector 32...\n",
      "    Using threshold=3 mask (15 pixels)\n",
      "      Extracted 3589 data points\n",
      "  Processing Sector 31...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 3445 data points\n",
      "  Processing Sector 29...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 3572 data points\n",
      "  Processing Sector 39...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 3865 data points\n",
      "  Processing Sector 36...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 3467 data points\n",
      "  Processing Sector 37...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 3461 data points\n",
      "  Processing Sector 34...\n",
      "    Using threshold=5 mask (15 pixels)\n",
      "      Extracted 3424 data points\n",
      "  Processing Sector 67...\n",
      "    Using threshold=3 mask (9 pixels)\n",
      "      Extracted 11490 data points\n",
      "  Processing Sector 66...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 11141 data points\n",
      "  Processing Sector 65...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 11662 data points\n",
      "  Processing Sector 69...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 9855 data points\n",
      "  Processing Sector 61...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 10653 data points\n",
      "  Processing Sector 63...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 10942 data points\n",
      "  Processing Sector 62...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 10811 data points\n",
      "  Processing Sector 64...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 11276 data points\n",
      "  Processing Sector 87...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 11168 data points\n",
      "  Processing Sector 93...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 10917 data points\n",
      "  Processing Sector 94...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 10356 data points\n",
      "  Processing Sector 96...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 10730 data points\n",
      "  Processing Sector 97...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 22640 data points\n",
      "  Processing Sector 88...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 11625 data points\n",
      "  Processing Sector 89...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 12144 data points\n",
      "  Processing Sector 90...\n",
      "    Using small circular aperture (r=2 pixels)\n",
      "      Extracted 11560 data points\n",
      "  Processing Sector 98...\n",
      "    Using threshold=5 mask (11 pixels)\n",
      "      Extracted 9020 data points\n",
      "\n",
      "  Stitching 37 sector(s) together...\n",
      "    Final: 244623 points, 2541.5 day baseline\n",
      "\n",
      "============================================================\n",
      "BOX LEAST SQUARES (BLS) SEARCH\n",
      "============================================================\n",
      "Detrending light curve...\n",
      "  Using known period 36.22d to set window\n",
      "  Flattening with window = 46945 cadences\n",
      "  After cleaning: 224336 points\n",
      "\n",
      "Constrained search: 28.98 - 43.47 days\n",
      "Running BLS...\n",
      "Estimating uncertainties...\n",
      "\n",
      "============================================================\n",
      "BLS RESULTS:\n",
      "============================================================\n",
      "  Period:   29.852926 Â± 0.055788 days\n",
      "  Duration: 7.920 Â± 0.792 hours\n",
      "  Depth:    13347.4 Â± 8211.3 ppm\n",
      "  BLS Power: 3480175.1\n",
      "  SNR: 2623.9\n",
      "  Transits: 85\n",
      "\n",
      "============================================================\n",
      "UNCERTAINTIES: BLS vs CATALOG\n",
      "============================================================\n",
      "\n",
      "BLS Results (with uncertainties):\n",
      "  Period:   29.852926 Â± 0.055788 days\n",
      "  Depth:    13347.4 Â± 8211.3 ppm\n",
      "  Duration: 7.920 Â± 0.792 hours\n",
      "\n",
      "Catalog Values:\n",
      "  Period:   36.222140 Â± 0.000460 days\n",
      "  Depth:    Not available\n",
      "  Duration: Not available\n",
      "\n",
      "------------------------------------------------------------\n",
      "Comparison:\n",
      "------------------------------------------------------------\n",
      "\n",
      "PERIOD:\n",
      "  Difference:     -6.369214 days (-17.58%)\n",
      "  Combined Error: 0.055790 days\n",
      "  Significance:   114.2 Ïƒ\n",
      "  Assessment:     âœ— Poor agreement\n",
      "============================================================\n",
      "    âœ“ Saved folded LC: 224336 points (P=29.8529d)\n",
      "  âœ“ P=29.853Â±0.0558d, Power=3480175\n",
      "\n",
      "======================================================================\n",
      "BATCH ANALYSIS FINISHED\n",
      "======================================================================\n",
      "\n",
      "Successfully analyzed 2/2 targets\n",
      "\n",
      "    toi    tic_id  bls_period  period_err    depth_ppm        power    verdict\n",
      "1029.01 374908020   29.852926    0.055788 13347.364950 3.480175e+06 SUSPICIOUS\n",
      "1025.01 297967252    8.759672    0.004261  1382.475026 2.898508e+05       GOOD\n",
      "\n",
      "======================================================================\n",
      "VETTING SUMMARY:\n",
      "======================================================================\n",
      "  âœ“ Good candidates: 1\n",
      "  âš  Suspicious: 1\n",
      "  âœ— Likely false positives: 0\n",
      "\n",
      "  Results saved to missing_radius_bls_results.csv\n",
      "  Successfully saved 2/2 folded light curves to 'test_lightcurves/'\n",
      "\n",
      "============================================================\n",
      "TEST COMPLETE!\n",
      "============================================================\n",
      "\n",
      "  Check the 'test_lightcurves/' folder for CSV files\n",
      "  Check 'missing_radius_bls_results.csv' for analysis results\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TESTING BATCH ANALYSIS ON 2 TOIs\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Search for TOIs with missing radius\n",
    "    df_missing = search_tois_with_missing_radius()\n",
    "    \n",
    "    if df_missing is not None and len(df_missing) > 0:\n",
    "        \n",
    "        print(\"\\n  First 5 candidates:\")\n",
    "        print(df_missing[['toi', 'tic_id', 'pl_orbper', 'st_tmag']].head().to_string(index=False))  # Changed tid to tic_id\n",
    "        \n",
    "        # Run batch analysis on just 2 TOIs\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RUNNING BATCH ANALYSIS ON 2 TOIs...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        results = batch_analyze_missing_radius(\n",
    "            df_missing, \n",
    "            num_candidates=2,  # Only analyze 2 TOIs\n",
    "            save_lcs=True,     # Save folded light curves\n",
    "            output_dir='test_lightcurves'  # Save to test directory\n",
    "        )\n",
    "        \n",
    "        if results is not None:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"TEST COMPLETE!\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"\\n  Check the 'test_lightcurves/' folder for CSV files\")\n",
    "            print(f\"  Check 'missing_radius_bls_results.csv' for analysis results\")\n",
    "        else:\n",
    "            print(\"\\n  âœ— Batch analysis failed\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\n  âœ— No TOIs with missing radius found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92364fce-3aa7-4380-84c9-99860f5bdd3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tess-env)",
   "language": "python",
   "name": "tess-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
